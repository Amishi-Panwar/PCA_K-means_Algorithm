# -*- coding: utf-8 -*-
"""PCA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/112LvYsrsCPzNa2msy4HQXqZHFmlBEbE0

**Que.1 PART 1**
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from numpy import linalg as LA
import sys


# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/gdrive')
# %cd /gdrive/My\ Drive

df = pd.read_csv("Assignment PRML/Dataset.csv",header=None)

df=np.array(df)
data_points=df.shape[0]  #No of points
features=df.shape[1]     #No of features

mean=np.zeros(2)
mean[0]=np.sum(df[:,0])/data_points
mean[1]=np.sum(df[:,1])/data_points
x=df-mean

C=(1/data_points)*(np.matmul(x.T,x))

values, vectorsc=LA.eig(C)

ordering = np.argsort(values,axis = 0)[::-1]
sorted_vectors = vectorsc[:,ordering]
sorted_values = values[ordering]
vector1c=vectorsc[:,1]
vector2c=vectorsc[:,0]

var1 = sorted_values[0]/np.sum(sorted_values)*100
var2 = sorted_values[1]/np.sum(sorted_values)*100
print("variance along principal component 1: ",var1)
print("variance along principal component 2: ",var2)

origin=[0,0]
plt.scatter(x[:,0],x[:,1])
PCA1 = plt.axline((0,0),vector1c,color="yellow",label="PCA 1")
PCA2 = plt.axline((0,0),vector2c,color="red",label="PCA 2")
plt.legend(handles=[PCA1, PCA2])
plt.xlabel("x-axis")
plt.ylabel("y-axis")
plt.title("Data points")
plt.show()

"""**Que.1 PART 2(Without Centering)**"""

#df is original dataset without centering
C=(1/data_points)*(np.matmul(df.T,df)) 
values, vectors=LA.eig(C)
ordering = np.argsort(values,axis = 0)[::-1]
sorted_vectors = vectors[:,ordering]
sorted_values = values[ordering]
vector1=vectors[:,1]
vector2=vectors[:,0]
var1 = sorted_values[0]/np.sum(sorted_values)*100
var2 = sorted_values[1]/np.sum(sorted_values)*100
plt.scatter(df[:,0],df[:,1])
PCA1 = plt.axline((0,0),vector1c,color="yellow",label="PCA 1")
PCA2 = plt.axline((0,0),vector2c,color="red",label="PCA 2")
plt.legend(handles=[PCA1, PCA2])
plt.xlabel("x-axis")
plt.ylabel("y-axis")
plt.show()

"""**Que.1 PART 3 KERNEL PCA**"""

def polyKernel(x,y,d):
    return (1+np.matmul(x,y.T))**d

"""**Part-3 (A) Polynomial kernel with degree 2** """

k=np.zeros((1000,1000))
for i in range(0,len(df)):
    for j in range(0,len(df)):
        k[i][j]= polyKernel(df[i,:],df[j,:],2)

# Centering K
oneMat = np.ones(len(k))/data_points
identityMat=np.identity(data_points)
k=np.matmul(np.matmul((identityMat-oneMat),k),(identityMat-oneMat))

eigen_values,eigen_vectors=LA.eig(k)
 
ordering = np.argsort(eigen_values,axis = 0)[::-1]
svectors = eigen_vectors[:,ordering]
seigenValues = eigen_values[ordering]

components=[]
for i in range(2):
    a=svectors[:,i]/((seigenValues[i])**(1/2))

    comp=[]
    for j in k:
        rowT=np.array(j).T
        comp.append(np.dot(rowT,a).real)
    components.append(comp)

plt.scatter(components[0],components[1])
plt.xlabel('x-axis')
plt.ylabel('y-axis')
plt.show()

"""**Part-3 (A) Polynomial kernel with degree 3)**"""

k=np.zeros((1000,1000))
for i in range(0,len(x)):
    for j in range(0,len(df)):
        k[i][j]= polyKernel(df[i,:],df[j,:],3)
        

# Centering K
oneMat = np.ones(len(k))/data_points
identityMat=np.identity(data_points)
k=np.matmul(np.matmul((identityMat-oneMat),k),(identityMat-oneMat))


eigen_values,eigen_vectors=LA.eig(k)

ordering = np.argsort(eigen_values,axis = 0)[::-1]
svectors = eigen_vectors[:,ordering]
seigenValues = eigen_values[ordering]

components=[]
for i in range(2):
    a=svectors[:,i]/((seigenValues[i])**(1/2))
    comp=[]
    for j in k:
        rowT=np.array(j).T
        comp.append(np.dot(rowT,a).real)
    components.append(comp)


plt.scatter(components[0],components[1])
plt.xlabel('x-axis')
plt.ylabel('y-axis')
plt.show()

"""**Part-3 (B) Radial Basis kernel with different sigma values)**"""

def radial_basis_function(x,y,d):
    return np.matrix(np.exp(np.matmul((y-x),(x-y).T)/(2*d*d)))

for y in range(1,11):
    k=np.zeros((1000,1000))
    for i in range(0,len(x)):
        for j in range(0,len(x)):
            k[i][j]= radial_basis_function(x[i,:],x[j,:],y/10)


    # Centering K
    oneMat = np.ones(len(k))/data_points

    identityMat=np.identity(data_points)
    k=np.matmul(np.matmul((identityMat-oneMat),k),(identityMat-oneMat))

    eigen_values,eigen_vectors=LA.eig(k)

    ordering = np.argsort(eigen_values,axis = 0)[::-1]
    svectors = eigen_vectors[:,ordering]
    seigenValues = eigen_values[ordering]

    components=[]
    for i in range(2):
        a=svectors[:,i]/((seigenValues[i])**(1/2))

        comp=[]
        for j in k:
            rowT=np.array(j).T
            comp.append(np.dot(rowT,a).real)
        components.append(comp)

    from matplotlib.pyplot import figure

    plt.scatter(components[0],components[1])
    plt.xlabel('x-axis')
    plt.ylabel('y-axis')
    plt.title('sigma: T=%1.1f' %(float(y/10)))
    plt.show()

"""***QUE.1 PART 4***

Polynomial kernel with degree 3 is best suited for this dataset because as we see from the plots polynomial kernel with degree 3 is giving projections which are similar to our dataset therefore gives less error.
"""
