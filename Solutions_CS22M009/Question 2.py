# -*- coding: utf-8 -*-
"""Clustering.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Hs26_DGyTencJHhtcaH57hbBetcehUk8

**Que.2 PART 1**
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from numpy import linalg as LA
import sys

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/gdrive')
# %cd /gdrive/My\ Drive

df = pd.read_csv("Assignment PRML/Dataset.csv",header=None)
df=np.array(df)
df

K=4
color=["red","blue","green","yellow","pink"]
data_points=df.shape[0]
features=df.shape[1]

for _ in range(5):
  z=[]
  for i in range(data_points):
    rand_point=np.random.randint(0,4)
    z.append(rand_point)
  clusters=[]
  for i in range(K):
      clusters.append([])
  for i in range(data_points):
      clusters[z[i]].append(i)

  centroids=np.zeros((K,features))
  error=[]
  iteration=[]
  for itr in range(500):
      flag=False
      for i in range(len(clusters)):
        sum=np.sum(df[clusters[i]],axis=0)
        if(len(clusters[i])!=0):
            new_mean=sum/len(clusters[i])
            centroids[i]=new_mean
      centroids=centroids[~np.all(centroids == 0, axis=1)]
  

      error_sum=0
      newclusters=[]
      for i in range(K):
          newclusters.append([])
      newz=[]

      for i in range(data_points):
          min_dist=sys.maxsize
          index=0
          for j in range(K):
              dist=np.sum((df[i]-centroids[j])**2)
              if(min_dist>dist):
                  index=j
                  min_dist=dist
          if(z[i]!=index):
            flag=True
          newclusters[index].append(i)
          newz.append(index)
          error_sum=error_sum+min_dist
      error.append(error_sum)
      iteration.append(itr)
      if(flag==False):
        break
      clusters=newclusters
      z=newz


  plt.plot(iteration,error)
  plt.xlabel('iteration')
  plt.ylabel('error')
  plt.title('Error function vs iteration')
  plt.show()

  for i in range(0,1000):
      plt.scatter(df[i][0],df[i][1],color=color[z[i]])
  
  plt.xlabel('x-axis')
  plt.ylabel('y-axis')
  plt.title('Clustering(k=4) with different initializations')
  plt.show()

"""**QUE.2 PART 2**"""

z_initial=[]
for i in range(data_points):
    rand_point=np.random.randint(0,1000)
    z_initial.append(rand_point)

k1=[2,3,4,5]
for itr in range(len(k1)):
  K=k1[itr]
  for i in range(data_points):
      z[i]=z_initial[i]%k1[itr]
  clusters=[]
  for i in range(K):
      clusters.append([])
  for i in range(data_points):
      clusters[z[i]].append(i)

  centroids=np.zeros((K,features))
  error=[]
  iteration=[]
  for itr in range(500):
      flag=False
      for i in range(len(clusters)):
        sum=np.sum(df[clusters[i]],axis=0)
        if(len(clusters[i])!=0):
            new_mean=sum/len(clusters[i])
            centroids[i]=new_mean
      centroids=centroids[~np.all(centroids == 0, axis=1)]

      error_sum=0
      newclusters=[]
      for i in range(K):
          newclusters.append([])
      newz=[]

      for i in range(data_points):
          min_dist=sys.maxsize
          index=0
          for j in range(K):
              dist=np.sum((df[i]-centroids[j])**2)
              if(min_dist>dist):
                  index=j
                  min_dist=dist
          if(z[i]!=index):
            flag=True
          newclusters[index].append(i)
          newz.append(index)
          error_sum=error_sum+min_dist
      error.append(error_sum)
      iteration.append(itr)
      if(flag==False):
        break
      clusters=newclusters
      z=newz

  xarr=np.arange(-10, 10, 0.5, dtype=float)
  yarr=np.arange(-10, 10, 0.5, dtype=float)
  znew=[]
  xplot=[]
  yplot=[]
  for i in range(len(xarr)):
    for j in range(len(yarr)):
      min_dist=sys.maxsize
      index=0
      sum=0
      for k in range(len(centroids)):
        sum=(xarr[i]-centroids[k][0])**2 +(yarr[j]-centroids[k][1])**2
        dist=np.sqrt(sum)
        if(min_dist>dist):
            index=k
            min_dist=dist
      xplot.append(xarr[i])
      yplot.append(yarr[j])
      znew.append(index)

  import seaborn as sns

  Data = {"x1":xplot,"x2":yplot,"z":znew}

  sns.scatterplot(data=Data,x="x1",y="x2",hue="z",palette="pastel")

  for i in range(0,1000):
      plt.scatter(df[i][0],df[i][1],color=color[z[i]])
  for i in range(len(centroids)):
    cenplot= plt.scatter(centroids[i][0],centroids[i][1],color="black")
  plt.xlabel('x-axis')
  plt.ylabel('y-axis')
  plt.title('Voronoi Regions')
  plt.show()

"""**QUE.3 PART 3**"""

x=df

def poly_kernel(x,y,d):
    return (1+np.matmul(x,y.T))**d
def radial_basis_function(x,y,d):
    return np.matrix(np.exp(np.matmul((y-x),(x-y).T)/(2*d*d)))

"""1. Polynomial Function (d=2,3)"""

degree=[2,3]
for d in degree:
  k=np.zeros((1000,1000))
  for i in range(0,len(x)):
      for j in range(0,len(x)):
          k[i][j]= poly_kernel(x[i,:],x[j,:],d)



  # Centering K
  oneMat = np.ones(len(k))/data_points
  identityMat=np.identity(data_points)
  k=np.matmul(np.matmul((identityMat-oneMat),k),(identityMat-oneMat))


  eigen_values,eigen_vectors=LA.eig(k)

  ordering = np.argsort(eigen_values,axis = 0)[::-1]
  svectors = eigen_vectors[:,ordering]
  seigenValues = eigen_values[ordering]

  H=[]

  for i in range(data_points):
      temp=[]
      for j in range(4):
          temp.append(0.0)
      H.append(temp)

  H=np.array(H)
  for i in range(4):
      H[:,i]=svectors[:,i].real

  newData=[]
  for row in H:
      newData.append(row)
      
  newData=np.array(newData)

  K=4
  data_points=newData.shape[0]
  features=newData.shape[1]
  z=[]
  for i in range(data_points):
    rand_point=np.random.randint(0,4)
    z.append(rand_point)
  clusters=[]
  for i in range(K):
      clusters.append([])
  for i in range(data_points):
      clusters[z[i]].append(i)

  centroids=np.zeros((K,features))
  error=[]
  iteration=[]
  for itr in range(500):
      flag=False
      for i in range(len(clusters)):
        sum=np.sum(newData[clusters[i]],axis=0)
        if(len(clusters[i])!=0):
            new_mean=sum/len(clusters[i])
            centroids[i]=new_mean
      centroids=centroids[~np.all(centroids == 0, axis=1)]

      error_sum=0
      newclusters=[]
      for i in range(K):
          newclusters.append([])
      newz=[]

      for i in range(data_points):
          min_dist=sys.maxsize
          index=0
          for j in range(K):
              dist=np.sum((newData[i]-centroids[j])**2)
              if(min_dist>dist):
                  index=j
                  min_dist=dist
          if(z[i]!=index):
            flag=True
          newclusters[index].append(i)
          newz.append(index)
          error_sum=error_sum+min_dist
      error.append(error_sum)
      iteration.append(itr)
      if(flag==False):
        break
      clusters=newclusters
      z=newz

  print("Error: ",error[-1])
  for i in range(0,1000):
      plt.scatter(x[i][0],x[i][1],color=color[z[i]])
  plt.xlabel('x-axis')
  plt.ylabel('y-axis')
  plt.title('Spectral Clustering')
  plt.show()

"""2. Radial Basis Function(sigma=0.1,0.2...1)"""

for y in range(1,11):
  k=np.zeros((1000,1000))
  for i in range(0,len(x)):
      for j in range(0,len(x)):
          k[i][j]= radial_basis_function(x[i,:],x[j,:],y/10)



  # Centering K
  oneMat = np.ones(len(k))/data_points
  identityMat=np.identity(data_points)
  k=np.matmul(np.matmul((identityMat-oneMat),k),(identityMat-oneMat))


  eigen_values,eigen_vectors=LA.eig(k)

  ordering = np.argsort(eigen_values,axis = 0)[::-1]
  svectors = eigen_vectors[:,ordering]
  seigenValues = eigen_values[ordering]

  H=[]

  for i in range(data_points):
      temp=[]
      for j in range(4):
          temp.append(0.0)
      H.append(temp)
 
  H=np.array(H)
  for i in range(4):
      H[:,i]=svectors[:,i].real

  newData=[]
  for row in H:
      newData.append(row)
      
  newData=np.array(newData)

  K=4
  data_points=newData.shape[0]
  features=newData.shape[1]
  z=[]
  for i in range(data_points):
    rand_point=np.random.randint(0,4)
    z.append(rand_point)
  clusters=[]
  for i in range(K):
      clusters.append([])
  for i in range(data_points):
      clusters[z[i]].append(i)

  centroids=np.zeros((K,features))
  error=[]
  iteration=[]
  for itr in range(500):
      flag=False
      for i in range(len(clusters)):
        sum=np.sum(newData[clusters[i]],axis=0)
        if(len(clusters[i])!=0):
            new_mean=sum/len(clusters[i])
            centroids[i]=new_mean
      centroids=centroids[~np.all(centroids == 0, axis=1)]

      error_sum=0
      newclusters=[]
      for i in range(K):
          newclusters.append([])
      newz=[]

      for i in range(data_points):
          min_dist=sys.maxsize
          index=0
          for j in range(K):
              dist=np.sum((newData[i]-centroids[j])**2)
              if(min_dist>dist):
                  index=j
                  min_dist=dist
          if(z[i]!=index):
            flag=True
          newclusters[index].append(i)
          newz.append(index)
          error_sum=error_sum+min_dist
      error.append(error_sum)
      iteration.append(itr)
      if(flag==False):
        break
      clusters=newclusters
      z=newz

  print("Error: ",error[-1])
  for i in range(0,1000):
      plt.scatter(x[i][0],x[i][1],color=color[z[i]])
  plt.xlabel('x-axis')
  plt.ylabel('y-axis')
  plt.title('Spectral Clustering')
  plt.show()

"""**QUE.2 PART 4**

Using Polynomial Kernel(d=2,3)
"""

degree=[2,3]
for d in degree:
  k=np.zeros((1000,1000))
  for i in range(0,len(x)):
      for j in range(0,len(x)):
          k[i][j]= radial_basis_function(x[i,:],x[j,:],d)

  # Centering K
  oneMat = np.ones(len(k))/data_points
  identityMat=np.identity(data_points)
  k=np.matmul(np.matmul((identityMat-oneMat),k),(identityMat-oneMat))


  eigen_values,eigen_vectors=LA.eig(k)

  ordering = np.argsort(eigen_values,axis = 0)[::-1]
  svectors = eigen_vectors[:,ordering]
  seigenValues = eigen_values[ordering]

  H=[]
  for i in range(data_points):
      temp=[]
      for j in range(4):
          temp.append(0.0)
      H.append(temp)

  H=np.array(H)
  for i in range(4):
      H[:,i]=svectors[:,i].real

  for i in range(data_points):
      maxIndex=np.argmax(H[i],axis=0)
      H[i][maxIndex]=1
      for j in range(4):
          if(j!=maxIndex):
              H[i][j]=0
  z=[]

  for i in range(data_points):
      for j in range(4):
          if(H[i][j]==1):
              z.append(j)

  for i in range(0,1000):
      plt.scatter(x[i][0],x[i][1],color=color[z[i]])
  plt.xlabel('x-axis')
  plt.ylabel('y-axis')
  plt.title('Clustering using polynomial function')
  plt.show()

"""Using Radial Basis Function(sigma=0.1,0.2...1)"""

for y in range(1,11):
  k=np.zeros((1000,1000))
  for i in range(0,len(x)):
      for j in range(0,len(x)):
          k[i][j]= radial_basis_function(x[i,:],x[j,:],y/10)

  # Centering K
  oneMat = np.ones(len(k))/data_points
  identityMat=np.identity(data_points)
  k=np.matmul(np.matmul((identityMat-oneMat),k),(identityMat-oneMat))


  eigen_values,eigen_vectors=LA.eig(k)

  ordering = np.argsort(eigen_values,axis = 0)[::-1]
  svectors = eigen_vectors[:,ordering]
  seigenValues = eigen_values[ordering]

  H=[]
  for i in range(data_points):
      temp=[]
      for j in range(4):
          temp.append(0.0)
      H.append(temp)

  H=np.array(H)
  for i in range(4):
      H[:,i]=svectors[:,i].real

  for i in range(data_points):
      maxIndex=np.argmax(H[i],axis=0)
      H[i][maxIndex]=1
      for j in range(4):
          if(j!=maxIndex):
              H[i][j]=0
  z=[]

  for i in range(data_points):
      for j in range(4):
          if(H[i][j]==1):
              z.append(j)

  
  for i in range(0,1000):
      plt.scatter(x[i][0],x[i][1],color=color[z[i]])
  plt.xlabel('x-axis')
  plt.ylabel('y-axis')
  plt.title('Clustering using radial basis function')
  plt.show()

